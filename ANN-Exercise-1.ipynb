{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D7046E Exercise 1 (ANN1)\n",
    "\n",
    "This exercise has three taks where you will deepen your understanding of how artificial neural networks (ANNs) are implemented and trained. First, you will represent digits on an eight-segment display as vectors and hard-code perceptrons that classifies the digits. The purpose of this task is to better understand the basic computational units in ANNs and how inputs can be represented as feature vectors. Secondly, you will implement and train neural networks using [pytorch](https://pytorch.org/) on the seven-segment display data and the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset which you are familiar with from Exercise 0. Finally, you will implement a neural network including the forward (inference) pass and the backward (learning) pass from scratch using numpy. After completing these steps you will know the central building blocks of ANNs.\n",
    "\n",
    "## Literature\n",
    "Before starting with the implementation you should familiarize yourself with two additional chapters in the [deep learning book](https://www.deeplearningbook.org/). This will help you understand the theory behind neural networks and what mathematical formulas are important for the task. The lectures has touched on most of these concepts. Below is a list of recommended sections from the book. If you feel familiar with the contents of these sections, feel free to skip it.\n",
    "\n",
    "* Chapter 6 - Deep feedforward networks\n",
    "    - Section 6.0 - Discusses what do we mean by feedfoward networks and terminology such as input layer, output layer and hidden layer.\n",
    "    - Section 6.2 - Discusses what gradient based learning is and what cost functions are.\n",
    "    - Section 6.5 - Explains back propagation. Important here are the formulas 6.49 - 6.52.\n",
    "* Chapter 8 - Optimization for Training Deep Models\n",
    "    - Section 8.1.3 - Presents differences between batch (deterministic) and mini-batch (stochastic) algorithms.\n",
    "    \n",
    "## Libraries\n",
    "\n",
    "Before starting with the implementations you need to import the following libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-09T10:35:40.465251700Z",
     "start_time": "2024-01-09T10:35:37.353754400Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import copy\n",
    "\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Task 1\n",
    "\n",
    "Classification of seven-segment display numbers with perceptrons. A [seven-segment display](https://en.wikipedia.org/wiki/Seven-segment_display) can be used to display digits by turning the different segments (A,B,C,D,E,F, G) on or off. Your task is to design ten different perceptrons (which together make a single-layer neural network with 10 outputs) that recognizes the ten different digits (0,1,2,3, ... ,9) represented by a seven-segment display. The input to each perceptron will be a seven-dimensional vector {A,B,C,D,E,F,G} where A is 1 if segment A is turned on and 0 otherwise (and the same for all the other segments).\n",
    "\n",
    "![Seven Segment Display](https://upload.wikimedia.org/wikipedia/commons/thumb/e/ed/7_Segment_Display_with_Labeled_Segments.svg/225px-7_Segment_Display_with_Labeled_Segments.svg.png)\n",
    "\n",
    "This means that for each digit you should create a vector representing the digit, as well as a perceptron that selectively gives a high output for that particular digit and a low output for other digits. For each digit (0 to 9) the corresponding perceptron shoud have the greatest output value, indicating that it can correctly classify the corresponding digit/vector and thereby distinguish that digit from other digits.\n",
    "\n",
    "For example, the digit 2 corresponds to the vector {1,1,0,1,1,0,1}. If the perceptron at index 2 (the third pereptron since we have a perceptron for 0 as well) gives the greatest output for this vector then this perceptron functions as desired. Similarly, other perceptrons should give the highest output for their corresponding vector.\n",
    "\n",
    "For example, the digit zero is represented as segments A-F being on and segment G being off. The zero perceptron should reasonably have positive activation for A-F, say at a value of 1, and negative activation for G, say -1.\n",
    "\n",
    "Why negative?\n",
    "\n",
    "If we instead set the activation for G to 0, then there is no distinction between the digits zero and eight. Thus, our perceptron for zero would have the same output regardless of whether the input is zero or eight. You can address this with biases, so that the output for our eight-perceptron is higher than the zero-perceptron for input eight, and vice versa, but it makes balancing weights and biases much more difficult.\n",
    "\n",
    "A good starting point is to consider which activations should be positive or negative, implement this, then check the output for all perceptrons for each digit and then contemplate the bias.\n",
    "\n",
    "\n",
    "### Task 1.1\n",
    "\n",
    "In this task you should use numpy rather than PyTorch to implement and understand the elementary arithmetic operations involved. After completing this exercise you should understand how an artificial neural network unit (like the perceptron) produces one scalar output from multiple input values, and how the trainable weights and biases determine the relation between input and output values of a neural network unit and layer.\n",
    "\n",
    "Complete the input vectors, the weight vectors, and biases. Then update the prediction calculation (forward pass) to include bias in the calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-09T10:35:56.967807200Z",
     "start_time": "2024-01-09T10:35:56.925697300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## First we need to define all the vectors corresponding to the various digits and add them to a list for easy access\n",
    "# Please finish the list of digit vectors\n",
    "x = [\n",
    "    numpy.array([1,1,1,1,1,1,0]), # 0\n",
    "    numpy.array([0,1,1,0,0,0,0]), # 1\n",
    "    numpy.array([1,1,0,1,1,0,1]), # 2\n",
    "    numpy.array([1,1,1,1,0,0,1]), # 3\n",
    "    numpy.array([0,1,1,0,0,1,1]), # 4\n",
    "    numpy.array([1,0,1,1,0,1,1]), # 5\n",
    "    numpy.array([1,0,1,1,1,1,1]), # 6\n",
    "    numpy.array([1,1,1,0,0,0,0]), # 7\n",
    "    numpy.array([1,1,1,1,1,1,1]), # 8\n",
    "    numpy.array([1,1,1,1,0,1,1]), # 9\n",
    "]\n",
    "\n",
    "# And we print one of the vectors to show you how to get a specific vector\n",
    "print(f'Digit 5 corresponds to the vector {x[5]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-09T10:36:37.574912200Z",
     "start_time": "2024-01-09T10:36:37.559272Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Second we need to create ten perceptron with weights and biases\n",
    "# You also need to figure out which weights and biases to use for each perceptron\n",
    "# We've already created some of the perceptrons for you, but you need to create the rest\n",
    "# While we're using integers for our weight you can use floating point numbers (real numbers) as well if you want\n",
    "\n",
    "weights = [\n",
    "    numpy.array([1,1,1,1,1,1,-1]),      # 0\n",
    "    numpy.array([-1,1,1,-1,-1,-1,-1]),  # 1\n",
    "    numpy.array([1,1,-1,1,1,-1,1]),     # 2\n",
    "    numpy.array([1,1,1,1,-1,-1,1]),     # 3\n",
    "    numpy.array([-1,1,1,-1,-1,1,1]),    # 4\n",
    "    numpy.array([1,-1,1,1,-1,1,1]),     # 5\n",
    "    numpy.array([1,-1,1,1,1,1,1]),      # 6\n",
    "    numpy.array([1,1,1,-1,-1,-1,-1]),   # 7\n",
    "    numpy.array([1,1,1,1,1,1,1]),       # 8\n",
    "    numpy.array([1,1,1,1,-1,1,1]),      # 9\n",
    "]\n",
    "\n",
    "biases = [\n",
    "    0,  # 0\n",
    "    1, # 1\n",
    "    0,  # 2\n",
    "    0,  # 3\n",
    "    0,  # 4\n",
    "    1,  # 5\n",
    "    0,  # 6\n",
    "    1,  # 7\n",
    "    -1,  # 8\n",
    "    0,   # 9\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-09T14:07:14.671398100Z",
     "start_time": "2024-01-09T14:07:14.669360Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Finally let's evaluate the output of the perceptrons.\n",
    "# The line computing the output of each perceptron is not using bias, so you need to add that as well.\n",
    "# If the perceptrons dont give the correct output then go back and edit the weights and biases in the\n",
    "# previous cell (or check that the vectors are implemented correctly) until the outputs discriminate\n",
    "# correctly between different digits. Remember to run a cell again (CONTROL + ENTER) if you update it.\n",
    "\n",
    "debug = False # Set this to True if the predictions are wrong to get a more detailed output\n",
    "\n",
    "for digit in range(10): # For each digit between 0 and 9 (range(n) gives a range (almost a list) of each number between 0 and n (excluding n)\n",
    "    vector = x[digit] # Get the correct vector representation of the digit\n",
    "    \n",
    "    outputs = [] # Create an empty list to put the perceptrons' outputs in\n",
    "    for w, b in zip(weights, biases): # For each weight and bias in the lists (zip takes two lists [x1,x2,...] [y1,y2,...] and makes a new list [(x1,y1),(x2,y2),...]) \n",
    "        \n",
    "        # CHANGE THIS LINE TO ADD BIASES AS WELL\n",
    "        output = w.dot(vector) + b # Calculating the output of the perceptron with weight w and bias b\n",
    "        \n",
    "        outputs.append(output) # Adding the output to the list of outputs\n",
    "    prediction = outputs.index(max(outputs)) # Get prediction by taking the index of the output value with maximum input\n",
    "    \n",
    "    print(f'Digit {digit} was predicted to be {prediction}') # This is an f-string with notation f'text {variable1} more text {variable2}'\n",
    "    \n",
    "    if debug: # If debug is True\n",
    "        print(f'Outputs for all perceptrons for the digit {digit}: {outputs}')\n",
    "        print() # Add a newline for formating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Task 1.2\n",
    "\n",
    "Next you will train a classifier for the seven-segment display digits using PyTorch, much like you did in Exercise 0 with another dataset. The goal is to learn the weights and biases to correctly classify the digits of a seven-segment display. For this task you have the data needed via the first task above. \n",
    "\n",
    "Create the network using the PyTorch model [torch.nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html), which you can find more information about in the [PyTorch documentation](https://pytorch.org/docs/stable/index.html). Whenever you encounter a new PyTorch function or class it's helpful to look it up in the documentation. If you wonder whether a particular machine learning feature exists in PyTorch you can also search in the documentation, or use the more brute force method of web search or asking an artificial intelligence for further information (see the discussion forum about AI in Canvas for further information). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-09T14:15:19.236289400Z",
     "start_time": "2024-01-09T14:15:19.226355400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Data and labels\n",
    "# You don't need to edit this code, we simply show you how, in this case, we create the data and labels we need\n",
    "\n",
    "# A matrix (or vector) in PyTorch is usually represented by a Tensor\n",
    "# Create a Tensor with our digit vectors\n",
    "data = torch.Tensor(x) # A Tensor can be created by simply giving it a nested numpy array/list of numbers as input\n",
    "data = data.detach() # Since we won't be chaning the data during training we detach the Tensor from the computation graph\n",
    "\n",
    "# Our labels will be the expected outputs of each perceptron for each digit\n",
    "# This means we can't simply say \"5\" is our expected output, but rather that we want [0,0,0,0,0,1,0,0,0,0]\n",
    "# This is called one-hot encoding where we have a vector where the value is one at the given index and zero everywhere else\n",
    "# Since a matrix with one-hot representations of the corresponding index is simply an identity matrix, we use that as our labels\n",
    "labels_train_raw = torch.eye(10) # Get a matrix with one-hot representations of each digit in each row (an identity matrix)\n",
    "labels_train_raw = labels_train_raw.detach() # Since we won't be changing the labels during training we detach the Tensor from the computation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## The network\n",
    "# We create a simple network from torch.nn.Linear\n",
    "\n",
    "input_size = 7 # What is the size of the input vector to the network?: # digits in 7-segment display\n",
    "output_size = 10 # What is the size of the output vector of the network?: # possible digits to display\n",
    "\n",
    "network = torch.nn.Linear(input_size, output_size) # Creating a single linear layer of a neural network with the given input and output sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Training the network\n",
    "# You don't need to edit this code, we have given you the training loop to train the network\n",
    "\n",
    "epochs = 100 # How many epochs (complete runs of the data) to train for. Since our dataset is small 100 seems reasonable\n",
    "loss_function = torch.nn.MSELoss() # What function to use to calculate the loss given the prediction and labels\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=1) # Function for updating the parameters of the network based on loss\n",
    "learning_rate = 1 # How fast to optimize the network. Since our problem is quite small we can have a large learning rate, otherise 0.01 is usually standard\n",
    "\n",
    "# Create a list to keep track of how the loss changes\n",
    "losses = []\n",
    "\n",
    "# For each epoch\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # Predict for each vector what digit they represent\n",
    "    prediction = network(data)\n",
    "    \n",
    "    # Calculate the loss of the prediction by comparing to the expected output\n",
    "    loss = loss_function(prediction, labels_train_raw)\n",
    "\n",
    "    # Backpropogate the loss through the network to find the gradients of all parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the parameters along their gradients\n",
    "    optimizer.step()\n",
    "\n",
    "    # Clear stored gradient values\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Add the loss to the total epoch loss (item() turns a PyTorch scalar into a normal Python datatype)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    #Print the epoch and loss every 10 epochs\n",
    "    if epoch % 10 == 9:\n",
    "        print(f'Epoch {epoch+1} - Loss: {loss}')\n",
    "    \n",
    "# Plot the training loss per epoch\n",
    "plt.plot(range(1,epochs+1),losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Task 1.3: Check the solution\n",
    "\n",
    "Execute the cell below to see whether the network managed to learn to make the correct predictions.\n",
    "Can you figure out what the learned weights and biases are, and how similar they are to your hardcoded solutions in the first task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Testing the trained network\n",
    "# You don't need to edit this code\n",
    "\n",
    "with torch.no_grad(): # Since we're not training we don't want to calculate the gradients for this prediction\n",
    "    prediction = network(data) # Let's make one final prediction of the data\n",
    "\n",
    "for digit in range(10):\n",
    "    print(f'Digit {digit} was predicted to be {torch.argmax(prediction[digit])}') # argmax gets the index with the greatest value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Task 2\n",
    "\n",
    "While it is unnecessary to use machine learning to develop classifiers for a seven-segment display there are many other datasets for which it is difficult, if not impossible, to engineer such solutions without data-driven training/optimization of a model. For example, we have no generally useful and efficient method enabling us to write a program that can tell the difference between images of cats and dogs. However, we can easily gather many images of each, label the images, and use the resulting dataset to train a neural network that solves the task. A central problem in machine learning is how to define and train such a model to obtain maximum performance on new data which has not been used to optimize the model, referred to as the generalization of the model. Another important problem in machine learning is to minimize the computational cost of training and using such models.\n",
    " \n",
    "Next you will extend the work started in Exercise 0 by developing a 2-layer neural network for classification of the MNIST dataset. MNIST consist of 70,000 grayscale images of handwritten digits that are 28x28 pixels each. Our goal is to train a 2-layer network that can recognize what digit an image represents. The subtasks are\n",
    "\n",
    "* Implementation of a 2 layer NN (very similar to ex0)\n",
    "* Training of this 2 layer NN (once again, very similar to ex0)\n",
    "* Validation of the network during training (requires splitting the training set)\n",
    "    - Save the model which performs the best on the validation data\n",
    "* Graph the training loss vs validation loss\n",
    "* You should obtain at least 85% accuracy on the test data (remember to load the best performing model before performing the accuracy test)\n",
    "\n",
    "The code below loads the dataset (downloads it if necessary) and displays one of the images. You have to modify this code to complete this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-09T14:42:41.322260300Z",
     "start_time": "2024-01-09T14:42:41.058524Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the mini-batch size (number of samples in each iteration)\n",
    "batch_size = 1000\n",
    "\n",
    "# Download the dataset and create the dataloaders\n",
    "mnist_train_raw = datasets.MNIST(\"./\", train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_test = datasets.MNIST(\"./\", train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "validation_split = 0.2  # 20%\n",
    "split = int(len(mnist_train_raw) * (1 - validation_split))\n",
    "train_dataset, val_dataset = random_split(mnist_train_raw, [split, len(mnist_train_raw) - split])\n",
    "\n",
    "# Create data loaders for training and validation\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# One-hot encoding embedding\n",
    "to_onehot = nn.Embedding(10, 10)\n",
    "to_onehot.weight.data = torch.eye(10)\n",
    "\n",
    "def plot_digit(data):\n",
    "    data = data.view(28, 28)\n",
    "    plt.imshow(data, cmap=\"gray\")\n",
    "    plt.show()\n",
    "\n",
    "images_train_raw, labels_train_raw = next(iter(train_loader))\n",
    "images_test, labels_test = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1\n",
    "\n",
    "Implement a 2-layer neural network using pytorch as well as a procedure for training and testing it. The training protocol should include both training and validation. Thus you need to split the training data into a training set (for which the error is backpropagated to update the parameters) and a validation set (which will not be used to directly update the model parameters, and instead be used to keep track of how good the model is at unseen data). \n",
    "\n",
    "The weights of the model which performs the best on the validation data should be stored and then be used for the final check on the test set. Validation sets are often created by taking a fraction of the training data (often, but not always, around 20%) at random. In Pytorch you might want to use [random_split](https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split) for this. Using random split would require you to edit the way the Dataloaders are created.\n",
    "\n",
    "You are free to choose any optimizer and loss function. Just note that some loss functions require the labels to be 1-hot encoded. As you will not use convolutional layers for this exercise (will be introduced later in the course), the inputs need to be transformed to 1d tensors (see [view](https://pytorch.org/docs/stable/tensors.html?highlight=view#torch.Tensor.view)).\n",
    "\n",
    "**GOAL:** You should evaluate the network from the epoch with best validation score (early stopping) on the test set aiming to reach at least 85% accuracy.\n",
    "\n",
    "**Remember** to run all your code before grading so that the teacher doesn't have to wait around for long training runs. Plot the training and validation losses for each epoch.\n",
    "\n",
    "*Hint:* Validation and Testing loops are very similar to training except they don't use backpropagation. Additionally testing should only be performed once, while validation should be performed continually to make sure training is proceeding as intended and to save the parameters of the best epoch.\n",
    "\n",
    "*Hint:* Storing the best model is more difficult than just assigning it to a variable as this only means you have two variables referencing the same network instance in memory (not a copy of the best betwork and one containing the current network). Instead you ned to make a copy of the network which can be achived with [deepcopy](https://docs.python.org/3/library/copy.html). Other ways to store models include saving them as a file which can be done with [torch.save](https://pytorch.org/tutorials/beginner/saving_loading_models.html).\n",
    "\n",
    "*Hint:* Everytime you train a network with random parameter initialization and random batches you get networks with different performance. Sometimes just running the training again can be enough to get a better result. However, if you do this too many times you run the risk of training (overfitting) on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Train Loss: 2.2175400257110596, Validation Loss: 2.218881825606028\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[92], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     mnist_network\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 21\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m784\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mto_onehot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataset.py:364\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataset.py:364\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\datasets\\mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    142\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\transforms\\functional.py:166\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# handle PIL Image\u001b[39;00m\n\u001b[0;32m    165\u001b[0m mode_to_nptype \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint32, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI;16\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint16, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mfloat32}\n\u001b[1;32m--> 166\u001b[0m img \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray(pic, mode_to_nptype\u001b[38;5;241m.\u001b[39mget(pic\u001b[38;5;241m.\u001b[39mmode, np\u001b[38;5;241m.\u001b[39muint8), copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    169\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m img\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Initialize the network, loss function, and optimizer\n",
    "mnist_network = nn.Sequential(\n",
    "    nn.Linear(28*28, 100), nn.ReLU(), nn.Linear(100, 10)\n",
    ")\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(mnist_network.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop with early stopping\n",
    "best_network = None\n",
    "best_val_loss = float('inf')\n",
    "train_losses, val_losses = [], []\n",
    "epochs = 100\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    mnist_network.train()\n",
    "    for images, labels in train_loader:\n",
    "        images = images.view(-1,784)\n",
    "        labels = to_onehot(labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = mnist_network(images)\n",
    "        loss = loss_function(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    mnist_network.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.view(-1,784)\n",
    "            labels = to_onehot(labels)\n",
    "            predictions = mnist_network(images)\n",
    "            val_loss += loss_function(predictions, labels).item()\n",
    "    val_loss /= len(val_loader)\n",
    "    train_losses.append(loss.item())\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    # Print epoch and losses\n",
    "    print(f'Epoch {epoch + 1}/{epochs} - Train Loss: {loss.item()}, Validation Loss: {val_loss}')\n",
    "\n",
    "    # Save the model with the best validation loss\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_network = torch.save(mnist_network.state_dict(), 'best_model.pth')\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    # Early stopping check\n",
    "    if patience_counter >= patience:\n",
    "        print(f'Early stopping. No improvement in validation loss for {patience} epochs.')\n",
    "        break\n",
    "\n",
    "\n",
    "# Load the best model parameters for testing\n",
    "mnist_network.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "# Test accuracy\n",
    "mnist_network.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(-1,784)\n",
    "        #labels = to_onehot(labels)\n",
    "        predictions = mnist_network(images)\n",
    "        _, predicted = torch.max(predictions.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total \n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Plot training and validation losses\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "Implement a 2-layer neural network like above, but now without using a high-level machine learning library like pytorch. The network should function in the same way as the network in Task 2. The code block below contains code to change the training data from the DataLoader format to the \"raw\" numpy format. It also contains some comments to guide you in the right direction. The substasks involved are\n",
    "\n",
    "* Implementation of a 2 layer NN using numpy\n",
    "* Training and validation of the 2 layer NN\n",
    "    - Once again, save the best performing model (can be done in memory)\n",
    "* Graph the training vs validation loss\n",
    "* At least 50% accuracy on the validation data (can be hard to get high accuracy)\n",
    "\n",
    "**Note that the solution does not need to be fast/scalable, thus it is OK to develop a custom solution with, e.g., two explicit weight matrix variables representing the two layers.**\n",
    "\n",
    "### The foward pass\n",
    "This is easy. Recall that each differnt layer is calculated by the formula: \n",
    "$$ y = g(\\mathbf{W}*\\mathbf{x} + b) $$\n",
    "where $W$ is the weight matrix, $x$ the input, $b$ the bias and $g$ the non-linearity. For this exercise you are allowed to put $b = 0$ for simplicity when calculating the backwards pass.\n",
    "\n",
    "### Backward pass\n",
    "This can be tricky. In canvas there is lecture material which explains back propogation and all the maths behind it. It should be under *Modules > Artificial Neural Networks (ANN) - Part 1 > Lecture: Backpropagation derivation.mp4*. This, the supplementary material for the lecture, together with the course book should be enough material for you to be able to implement the training algorithm.\n",
    "\n",
    "### Weight update\n",
    "Once you have calculated the gradient of both weight matrixes, this is updated by:\n",
    "$$ W_i = W_i - \\gamma \\dfrac{dL}{dW_i} $$\n",
    "where $\\gamma$ is the step size, or learning rate.\n",
    "\n",
    "***Remember*** to run all your code before grading so the teacher doesn't have to wait around for long training runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Train Loss: 2.3331962169595715e+119, Validation Loss: 0.33745156970806156\n",
      "Epoch 2/100 - Train Loss: 0.11899294715307414, Validation Loss: 0.3202675651508097\n",
      "Epoch 3/100 - Train Loss: 0.10669487420187253, Validation Loss: 0.3150122036183859\n",
      "Epoch 4/100 - Train Loss: 0.10304818959747247, Validation Loss: 0.3123755314933989\n",
      "Epoch 5/100 - Train Loss: 0.10152043554823748, Validation Loss: 0.3108328755648202\n",
      "Epoch 6/100 - Train Loss: 0.10078789629070403, Validation Loss: 0.30988686267970283\n",
      "Epoch 7/100 - Train Loss: 0.10040749545835427, Validation Loss: 0.3092810229833899\n",
      "Epoch 8/100 - Train Loss: 0.10019866395686974, Validation Loss: 0.30885702543091104\n",
      "Epoch 9/100 - Train Loss: 0.10007275623309436, Validation Loss: 0.3085552846623792\n",
      "Epoch 10/100 - Train Loss: 0.09999487129142462, Validation Loss: 0.30836264523498913\n",
      "Epoch 11/100 - Train Loss: 0.09994447860821876, Validation Loss: 0.30821604229605276\n",
      "Epoch 12/100 - Train Loss: 0.09991094543820998, Validation Loss: 0.30810190956311456\n",
      "Epoch 13/100 - Train Loss: 0.09988801713604241, Validation Loss: 0.30801168073196833\n",
      "Epoch 14/100 - Train Loss: 0.09987191363426895, Validation Loss: 0.30794071149521357\n",
      "Epoch 15/100 - Train Loss: 0.09986030508096608, Validation Loss: 0.30788330996429963\n",
      "Epoch 16/100 - Train Loss: 0.09985172777222387, Validation Loss: 0.30783637132518543\n",
      "Epoch 17/100 - Train Loss: 0.09984521837435834, Validation Loss: 0.30779619150891663\n",
      "Epoch 18/100 - Train Loss: 0.09983913304853173, Validation Loss: 0.3077623493272543\n",
      "Epoch 19/100 - Train Loss: 0.09983417658507772, Validation Loss: 0.3077335777659651\n",
      "Epoch 20/100 - Train Loss: 0.09983009749530067, Validation Loss: 0.307708906341662\n",
      "Epoch 21/100 - Train Loss: 0.09982671341713249, Validation Loss: 0.3076875846209142\n",
      "Epoch 22/100 - Train Loss: 0.09982388820958801, Validation Loss: 0.30766902693916015\n",
      "Epoch 23/100 - Train Loss: 0.0998215176052145, Validation Loss: 0.3076527720413228\n",
      "Epoch 24/100 - Train Loss: 0.09981952001452635, Validation Loss: 0.30763845335210266\n",
      "Epoch 25/100 - Train Loss: 0.09981783047555175, Validation Loss: 0.307625776899374\n",
      "Epoch 26/100 - Train Loss: 0.09981639933810024, Validation Loss: 0.30761528540607924\n",
      "Epoch 27/100 - Train Loss: 0.09981518016162923, Validation Loss: 0.3076058116154331\n",
      "Epoch 28/100 - Train Loss: 0.09981413787365209, Validation Loss: 0.3075972431969567\n",
      "Epoch 29/100 - Train Loss: 0.09981324375769969, Validation Loss: 0.3075894830275366\n",
      "Epoch 30/100 - Train Loss: 0.09981247387108093, Validation Loss: 0.30758244661496065\n",
      "Epoch 31/100 - Train Loss: 0.09981180821143078, Validation Loss: 0.30757606004777827\n",
      "Epoch 32/100 - Train Loss: 0.09981123004258093, Validation Loss: 0.30757025834827934\n",
      "Epoch 33/100 - Train Loss: 0.09981072534465694, Validation Loss: 0.30756498413730576\n",
      "Epoch 34/100 - Train Loss: 0.09981028236248426, Validation Loss: 0.307560186542555\n",
      "Epoch 35/100 - Train Loss: 0.09980989123276574, Validation Loss: 0.3075558202987217\n",
      "Epoch 36/100 - Train Loss: 0.09980954367501993, Validation Loss: 0.30755184500006033\n",
      "Epoch 37/100 - Train Loss: 0.09980923273455569, Validation Loss: 0.30754822447502483\n",
      "Epoch 38/100 - Train Loss: 0.09980895256819074, Validation Loss: 0.307544926259435\n",
      "Epoch 39/100 - Train Loss: 0.09980869826526272, Validation Loss: 0.30754192114974105\n",
      "Epoch 40/100 - Train Loss: 0.09980846569789727, Validation Loss: 0.3075391828218633\n",
      "Epoch 41/100 - Train Loss: 0.09980825139560524, Validation Loss: 0.30753668750408264\n",
      "Epoch 42/100 - Train Loss: 0.09980805244016168, Validation Loss: 0.30753441369476614\n",
      "Epoch 43/100 - Train Loss: 0.09980786637742334, Validation Loss: 0.3075323419175284\n",
      "Epoch 44/100 - Train Loss: 0.09980769114331439, Validation Loss: 0.3075304545078289\n",
      "Epoch 45/100 - Train Loss: 0.09980752500167629, Validation Loss: 0.30752873542613374\n",
      "Epoch 46/100 - Train Loss: 0.0998073664920618, Validation Loss: 0.30752717009364494\n",
      "Epoch 47/100 - Train Loss: 0.09980721438586918, Validation Loss: 0.3075257452473128\n",
      "Epoch 48/100 - Train Loss: 0.09980706764947628, Validation Loss: 0.30752444881140667\n",
      "Epoch 49/100 - Train Loss: 0.09980692541325102, Validation Loss: 0.3075232697833809\n",
      "Epoch 50/100 - Train Loss: 0.09980678694549804, Validation Loss: 0.3075221981321363\n",
      "Epoch 51/100 - Train Loss: 0.09980665163055209, Validation Loss: 0.30752122470708426\n",
      "Epoch 52/100 - Train Loss: 0.09980651895035618, Validation Loss: 0.3075203411566603\n",
      "Epoch 53/100 - Train Loss: 0.09980638846896894, Validation Loss: 0.30751953985514097\n",
      "Epoch 54/100 - Train Loss: 0.09980625981953413, Validation Loss: 0.30751881383678176\n",
      "Epoch 55/100 - Train Loss: 0.09980613269332027, Validation Loss: 0.3075181567364335\n",
      "Epoch 56/100 - Train Loss: 0.09980600683050106, Validation Loss: 0.30751756273591235\n",
      "Epoch 57/100 - Train Loss: 0.09980588201239982, Validation Loss: 0.30751702651548807\n",
      "Epoch 58/100 - Train Loss: 0.09980575805496465, Validation Loss: 0.3075165432099488\n",
      "Epoch 59/100 - Train Loss: 0.0998056348032795, Validation Loss: 0.30751610836875604\n",
      "Epoch 60/100 - Train Loss: 0.09980551212694576, Validation Loss: 0.30751571791987503\n",
      "Epoch 61/100 - Train Loss: 0.09980538991619677, Validation Loss: 0.30751536813690444\n",
      "Epoch 62/100 - Train Loss: 0.09980526807862843, Validation Loss: 0.3075150556091823\n",
      "Epoch 63/100 - Train Loss: 0.0998051465364485, Validation Loss: 0.30751477721457426\n",
      "Epoch 64/100 - Train Loss: 0.0998050252241616, Validation Loss: 0.3075145300946879\n",
      "Epoch 65/100 - Train Loss: 0.09980490408662186, Validation Loss: 0.30751431163228143\n",
      "Epoch 66/100 - Train Loss: 0.09980478307739359, Validation Loss: 0.3075141194306617\n",
      "Epoch 67/100 - Train Loss: 0.0998046621573722, Validation Loss: 0.3075139512948863\n",
      "Epoch 68/100 - Train Loss: 0.0998045412936236, Validation Loss: 0.307513805214605\n",
      "Epoch 69/100 - Train Loss: 0.09980442045840716, Validation Loss: 0.3075136793483878\n",
      "Epoch 70/100 - Train Loss: 0.09980429962835409, Validation Loss: 0.30751357200941176\n",
      "Epoch 71/100 - Train Loss: 0.09980417878377529, Validation Loss: 0.307513481652378\n",
      "Epoch 72/100 - Train Loss: 0.09980405790807934, Validation Loss: 0.3075134068615535\n",
      "Epoch 73/100 - Train Loss: 0.09980393698728235, Validation Loss: 0.30751334633983696\n",
      "Epoch 74/100 - Train Loss: 0.09980381600959569, Validation Loss: 0.3075132988987599\n",
      "Epoch 75/100 - Train Loss: 0.09980369496507872, Validation Loss: 0.30751326344933766\n",
      "Epoch 76/100 - Train Loss: 0.09980357384534672, Validation Loss: 0.3075132389937011\n",
      "Epoch 77/100 - Train Loss: 0.09980345264332494, Validation Loss: 0.30751322461743674\n",
      "Epoch 78/100 - Train Loss: 0.0998033313530416, Validation Loss: 0.307513219482578\n",
      "Epoch 79/100 - Train Loss: 0.09980320996945372, Validation Loss: 0.307513222821188\n",
      "Epoch 80/100 - Train Loss: 0.09980308848830038, Validation Loss: 0.3075132339294857\n",
      "Epoch 81/100 - Train Loss: 0.09980296690597931, Validation Loss: 0.3075132521624674\n",
      "Epoch 82/100 - Train Loss: 0.0998028452194429, Validation Loss: 0.3075132769289815\n",
      "Epoch 83/100 - Train Loss: 0.09980272342611078, Validation Loss: 0.3075133076872192\n",
      "Epoch 84/100 - Train Loss: 0.09980260152379625, Validation Loss: 0.3075133439405833\n",
      "Epoch 85/100 - Train Loss: 0.09980247951064404, Validation Loss: 0.3075133852339043\n",
      "Epoch 86/100 - Train Loss: 0.0998023573850783, Validation Loss: 0.3075134311499768\n",
      "Epoch 87/100 - Train Loss: 0.09980223514575866, Validation Loss: 0.3075134813063826\n",
      "Epoch 88/100 - Train Loss: 0.09980211279154307, Validation Loss: 0.3075135353525844\n",
      "Epoch 89/100 - Train Loss: 0.09980199032145684, Validation Loss: 0.3075135929672604\n",
      "Epoch 90/100 - Train Loss: 0.09980186773466632, Validation Loss: 0.30751365385586354\n",
      "Epoch 91/100 - Train Loss: 0.09980174503045675, Validation Loss: 0.3075137177483851\n",
      "Epoch 92/100 - Train Loss: 0.09980162220821381, Validation Loss: 0.30751378439730503\n",
      "Epoch 93/100 - Train Loss: 0.09980149926740785, Validation Loss: 0.30751385357571487\n",
      "Epoch 94/100 - Train Loss: 0.09980137620758076, Validation Loss: 0.3075139250755958\n",
      "Epoch 95/100 - Train Loss: 0.09980125302833479, Validation Loss: 0.3075139987062432\n",
      "Epoch 96/100 - Train Loss: 0.09980112972932333, Validation Loss: 0.30751407429282096\n",
      "Epoch 97/100 - Train Loss: 0.09980100631024291, Validation Loss: 0.30751415167503765\n",
      "Epoch 98/100 - Train Loss: 0.09980088277082663, Validation Loss: 0.30751423070593253\n",
      "Epoch 99/100 - Train Loss: 0.09980075911083862, Validation Loss: 0.3075143112507643\n",
      "Epoch 100/100 - Train Loss: 0.09980063533006917, Validation Loss: 0.3075143931859909\n",
      "Test Accuracy: 7.80%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def np_ReLU(x):\n",
    "    return numpy.maximum(0, x)\n",
    "\n",
    "def np_der_ReLU(x):\n",
    "    if numpy.all(x) == False: raise Exception(\"0 can not be derived in ReLU\")\n",
    "    return numpy.where(x > 0, 1, 0)\n",
    "\n",
    "# Random data generation for training and validation\n",
    "numpy.random.seed(42)  # for reproducibility\n",
    "train_data_np, train_labels_np = next(iter(train_loader))\n",
    "val_data_np, val_labels_np = next(iter(val_loader))\n",
    "test_data_np, test_labels_np = next(iter(test_loader))\n",
    "\n",
    "# Preprocesing\n",
    "train_data_np = train_data_np.reshape((-1, 28*28)).numpy()\n",
    "train_labels_np = train_labels_np.numpy()\n",
    "val_data_np = val_data_np.reshape((-1, 28*28)).numpy()\n",
    "val_labels_np = val_labels_np.numpy()\n",
    "test_data_np = test_data_np.view(-1,784).numpy()\n",
    "test_labels_np = test_labels_np.numpy()\n",
    "\n",
    "# Set the number of epochs, input size, output size, hidden layer size, and learning rate\n",
    "epochs = 100  # Set the number of epochs to train for\n",
    "D_in = 784   # Input size, images are 28x28 = 784 element vectors\n",
    "D_out = 10   # Output size, 10 digit classes\n",
    "H1 = 100     # Hidden layer size\n",
    "gamma = 1e-5 # Learning rate\n",
    "\n",
    "# Define network with one hidden layer, random initial weights\n",
    "w1 = numpy.random.randn(D_in, H1)\n",
    "w2 = numpy.random.randn(H1, D_out)\n",
    "\n",
    "# Training iterations\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "best_model = {'w1': w1.copy(), 'w2': w2.copy(), 'accuracy': 0}  # to keep track of the best model's weights and accuracy\n",
    "\n",
    "# Train for a number of epochs\n",
    "for epoch in range(epochs):\n",
    "    # Clear training loss at the beginning of each epoch\n",
    "    train_loss = 0\n",
    "\n",
    "    # Training by looping over training set\n",
    "    for i in range(len(train_data_np)):\n",
    "        # Iterate through the training set and perform forward pass and backward pass\n",
    "        x, y = train_data_np[i], train_labels_np[i]\n",
    "        x = x.flatten().reshape((1, D_in))\n",
    "        y = numpy.eye(10)[y]  # 1-hot encoding\n",
    "\n",
    "        # Forward pass\n",
    "        h = x.dot(w1)\n",
    "        h_relu = np_ReLU(h)\n",
    "        y_pred = h_relu.dot(w2)\n",
    "\n",
    "        # Compute loss function, squared error\n",
    "        mse = numpy.mean(numpy.square(y - y_pred))\n",
    "        train_loss += mse\n",
    "\n",
    "        # Compute gradients of square-error loss with respect to w1 and w2 using backpropagation\n",
    "        Lp = -2.0 * (y - y_pred)\n",
    "        dLw2 = h_relu.T.dot(Lp)\n",
    "        dLw1 = x.T.dot((Lp.dot(w2.T)) * np_der_ReLU(h))\n",
    "\n",
    "        # Update weights (stochastic gradient descent)\n",
    "        # Wi <- Wi - gamma * (dL/dWi)\n",
    "        w1 = w1 - gamma * dLw1\n",
    "        w2 = w2 - gamma * dLw2\n",
    "\n",
    "    # Log training loss\n",
    "    train_loss /= len(train_data_np)\n",
    "    train_loss_list.append(train_loss)\n",
    "\n",
    "    # Validate the model\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    val_loss = 0.0\n",
    "    for i in range(len(val_data_np)):\n",
    "        # Iterate through the validation set and perform forward pass (no backward pass in validation)\n",
    "        x, y = val_data_np[i], val_labels_np[i]\n",
    "        x = x.flatten().reshape((1, D_in))\n",
    "        y_val = numpy.eye(10)[y]\n",
    "\n",
    "        # Forward pass (same as train)\n",
    "        h_val = x.dot(w1)\n",
    "        h_relu_val = np_ReLU(h_val)\n",
    "        y_pred_val = h_relu_val.dot(w2)\n",
    "\n",
    "        # Compute validation loss\n",
    "        val_loss += numpy.mean(numpy.square(y_val - y_pred_val))\n",
    "        total += 1\n",
    "        correct += numpy.argmax(y_pred_val) == numpy.argmax(y_val)\n",
    "\n",
    "    # Calculate accuracy on the validation set\n",
    "    accuracy = correct / total\n",
    "    print(f'Epoch {epoch + 1}/{epochs} - Train Loss: {train_loss}, Validation Loss: {val_loss / len(val_data_np)}')\n",
    "\n",
    "    # Save the best model\n",
    "    if accuracy > best_model['accuracy']:\n",
    "        best_model['w1'] = w1.copy()\n",
    "        best_model['w2'] = w2.copy()\n",
    "        best_model['accuracy'] = accuracy\n",
    "    val_loss_list.append(val_loss / len(val_data_np))\n",
    "\n",
    "# Test the best model on the test set\n",
    "total = 0\n",
    "correct = 0\n",
    "for i in range(len(test_data_np)):\n",
    "    # Iterate through the test set and perform forward pass (no backward pass in test)\n",
    "    x, y = test_data_np[i], test_labels_np[i]\n",
    "    x = x.flatten().reshape((1, D_in))\n",
    "\n",
    "    h_test = x.dot(best_model['w1'])\n",
    "    h_relu_test = np_ReLU(h_test)\n",
    "    y_pred_test =h_relu_test.dot(best_model['w2'])\n",
    "\n",
    "    total += 1\n",
    "    correct += int(numpy.argmax(y_pred_test) == y)\n",
    "\n",
    "# Calculate accuracy on the test set\n",
    "test_accuracy = correct / total\n",
    "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
    "\n",
    "# Plot training and validation losses\n",
    "plt.plot(train_loss_list, label='Training Loss')\n",
    "plt.plot(val_loss_list, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## End\n",
    "\n",
    "You have now reached the end of ANN1. When you have completed and understood the task above please make sure that all results inluding plots have been computed and then schedule a meeting with a teacher. The teacher will then assess orally that you (the lab group) has completed the exercise and that you understand its essental elements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
